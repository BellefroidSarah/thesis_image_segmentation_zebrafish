{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f9718d",
   "metadata": {},
   "source": [
    "# Threshold study\n",
    "Evaluates the performances of the binary or cropped for different threshold values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a7eec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "\n",
    "import utils\n",
    "import datasets\n",
    "import metrics\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import constants as cst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b58d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_name = \"Performances of the model for different thresholds\"\n",
    "\n",
    "# All fold per model run\n",
    "model0 = \"vc_Dice_Fold_0_Epoch_246_MaxEpochs_250_Adam_LR_0.0001.pth\"\n",
    "model1 = \"vc_Dice_Fold_1_Epoch_248_MaxEpochs_250_Adam_LR_0.0001.pth\"\n",
    "model2 = \"vc_Dice_Fold_2_Epoch_249_MaxEpochs_250_Adam_LR_0.0001.pth\"\n",
    "model3 = \"vc_Dice_Fold_3_Epoch_247_MaxEpochs_250_Adam_LR_0.0001.pth\"\n",
    "model4 = \"vc_Dice_Fold_4_Epoch_249_MaxEpochs_250_Adam_LR_0.0001.pth\"\n",
    "\n",
    "models = [model0, model1, model2, model3, model4]\n",
    "\n",
    "DEVICE_NAME = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE_NAME = 'cuda:0'\n",
    "DEVICE = torch.device(DEVICE_NAME)\n",
    "\n",
    "term = \"vc\"\n",
    "# Cropped model or binary\n",
    "crop = False\n",
    "\n",
    "n_th = 51\n",
    "thresholds = np.linspace(0, 1, num=n_th)\n",
    "\n",
    "fold = 0\n",
    "image_folder = \"/notebooks/images\"\n",
    "mask_folder = \"/notebooks/\" + term\n",
    "\n",
    "if crop:\n",
    "    model_name = \"fish_CE_Fold_4_Epoch_95_MaxEpochs_600_Adam_LR_0.0001.pth\"\n",
    "    model_path = os.path.join(cst.MODEL, model_name)\n",
    "\n",
    "    model = utils.load_model(model_path)\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    testing_set = datasets.ZebrafishDataset_KFold_crop_head(image_folder,\n",
    "                                                           mask_folder,\n",
    "                                                           actual_fold=fold,\n",
    "                                                           model = model,\n",
    "                                                           device = DEVICE,\n",
    "                                                           dataset=\"test\",\n",
    "                                                           folds = cst.FOLDS)    \n",
    "else:\n",
    "    testing_set = datasets.ZebrafishDataset_KFold(image_folder,\n",
    "                                                     mask_folder,\n",
    "                                                     actual_fold=fold,\n",
    "                                                     dataset=\"test\",\n",
    "                                                     folds=cst.FOLDS)\n",
    "    \n",
    "testing_loader = torch.utils.data.DataLoader(testing_set,\n",
    "                                             batch_size=1,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=cst.WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14295448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_img(model, image, device, transform, out_threshold=0.5):\n",
    "    with torch.no_grad():\n",
    "        x = image\n",
    "        logits = model(x.to(device))\n",
    "        logits = transform(logits)\n",
    "        y_pred = nn.Softmax(dim=1)(logits)\n",
    "        proba = y_pred.detach().cpu().squeeze(0).numpy()[1, :, :]\n",
    "        return proba > out_threshold\n",
    "    \n",
    "# Evaluate the performances for the binary model\n",
    "def evaluate(eval_model, testing_loader, threshold):\n",
    "    tps = 0\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    F1s = []\n",
    "    IOUs = []\n",
    "    \n",
    "    SIZE = (384, 512)\n",
    "    \n",
    "    transform = transforms.Compose([transforms.Resize(SIZE),\n",
    "                                    transforms.Pad((0, 64, 0, 64))])\n",
    "    untransform = transforms.Compose([transforms.CenterCrop(SIZE),\n",
    "                                     transforms.Resize((1932, 2576))])\n",
    "\n",
    "    eval_model.eval()\n",
    "    for image, mask, name in testing_loader:\n",
    "        \n",
    "        prediction = utils.predict_img(eval_model, transform(image), DEVICE, untransform, out_threshold=threshold)\n",
    "        pred = torch.from_numpy(prediction)\n",
    "\n",
    "        precisions.append(metrics.precision(pred, mask))\n",
    "        recalls.append(metrics.recall(pred, mask))\n",
    "        F1s.append(metrics.F1Score(pred, mask))\n",
    "        IOUs.append(metrics.IOUScore(pred, mask))\n",
    "    return precisions, recalls, F1s, IOUs\n",
    "\n",
    "# Evaluate the performances for the cropped model\n",
    "def evaluate_crop(eval_model, testing_loader, threshold):\n",
    "    tps = 0\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    F1s = []\n",
    "    IOUs = []\n",
    "    \n",
    "    eval_model.eval()\n",
    "    for image, mask, name, size in testing_loader:\n",
    "        h_length = int(size[0])\n",
    "        v_length = int(size[1])\n",
    "        if h_length>v_length:\n",
    "            untr = transforms.Compose([transforms.Resize((h_length, h_length)),\n",
    "                                       transforms.CenterCrop((v_length, h_length))])\n",
    "        elif h_length<v_length:\n",
    "            untr = transforms.Compose([transforms.Resize((v_length, v_length)),\n",
    "                                       transforms.CenterCrop((v_length, h_length))])\n",
    "        else:\n",
    "            untr = transforms.Compose([transforms.Resize((h_length, h_length))])\n",
    "\n",
    "        image_name = name[0]\n",
    "        \n",
    "        prediction = utils.predict_img(eval_model, image, DEVICE, untr, out_threshold=threshold)\n",
    "        pred = torch.from_numpy(prediction)\n",
    "        mask = untr(mask)\n",
    "\n",
    "        precisions.append(metrics.precision(pred, mask))\n",
    "        recalls.append(metrics.recall(pred, mask))\n",
    "        F1s.append(metrics.F1Score(pred, mask))\n",
    "        IOUs.append(metrics.IOUScore(pred, mask))\n",
    "    return precisions, recalls, F1s, IOUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bc62e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "precisions = []\n",
    "recalls = []\n",
    "F1s = []\n",
    "IOUs = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    m_precision = []\n",
    "    m_recall = []\n",
    "    m_f1 = []\n",
    "    m_iou = []\n",
    "    \n",
    "    for model_name in models:\n",
    "        if crop:\n",
    "            m_dir = term+\"_cropped\"\n",
    "        else:\n",
    "            m_dir = term+\"_normal\"\n",
    "        model_path = os.path.join(cst.DIR, m_dir, model_name)\n",
    "        model = utils.load_model(model_path)\n",
    "        model.to(DEVICE)\n",
    "        \n",
    "        if crop:\n",
    "            p,r,f,i = evaluate_crop(model, testing_loader, threshold)           \n",
    "        else:\n",
    "            p,r,f,i = evaluate(model, testing_loader, threshold)\n",
    "            \n",
    "        m_precision.append(np.mean(p))\n",
    "        m_recall.append(np.mean(r))\n",
    "        m_f1.append(np.mean(f))\n",
    "        m_iou.append(np.mean(i))\n",
    "    \n",
    "    precisions.append(np.mean(m_precision))\n",
    "    recalls.append(np.mean(m_recall))\n",
    "    F1s.append(np.mean(m_f1))\n",
    "    IOUs.append(np.mean(m_iou))\n",
    "    \n",
    "means = []\n",
    "for i in range(n_th):\n",
    "    m = (precisions[i]+recalls[i]+F1s[i]+IOUs[i])/4\n",
    "    means.append(m)\n",
    "    \n",
    "    \n",
    "TERM = term\n",
    "loss_name = \"Dice\"\n",
    "run_name = model_dir\n",
    "dir_name = os.path.join(cst.DIR, run_name)\n",
    "\n",
    "    \n",
    "# Plot - all metric curves\n",
    "plt.plot(thresholds, means , label=\"average\", color=\"tab:blue\")\n",
    "plt.plot(thresholds, precisions , label=\"precision\", color=\"tab:orange\")\n",
    "plt.plot(thresholds, recalls, label=\"recall\", color=\"tab:green\")\n",
    "plt.plot(thresholds, F1s, label=\"F1\", color=\"tab:red\")\n",
    "plt.plot(thresholds, IOUs, label=\"IOU\", color=\"tab:purple\")\n",
    "plt.ylabel(\"Metrics\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.title(\"Term: \" + TERM + \", Loss: \" + loss_name)\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(dir_name, TERM + \"_\" + loss_name + \"_Metric_curves.jpg\"))\n",
    "plt.show()\n",
    "\n",
    "# Plot - average curve\n",
    "best = np.argmax(means)\n",
    "plt.plot(thresholds, means , label=\"average\", color=\"tab:blue\")\n",
    "plt.vlines(thresholds[best], 0, means[best], colors=\"black\",linestyles=\"dashed\")\n",
    "plt.hlines(means[best], 0, thresholds[best], colors=\"black\",linestyles=\"dashed\")\n",
    "plt.ylabel(\"Average\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.title(\"Term: \" + TERM + \", Loss: \" + loss_name)\n",
    "plt.savefig(os.path.join(dir_name, TERM + \"_\" + loss_name + \"_average_curves.jpg\"))\n",
    "plt.show()\n",
    "\n",
    "# Plot - precision curve\n",
    "best = np.argmax(precisions)\n",
    "plt.plot(thresholds, precisions , label=\"precision\", color=\"tab:orange\")\n",
    "plt.vlines(thresholds[best], 0, precisions[best], colors=\"black\",linestyles=\"dashed\")\n",
    "plt.hlines(precisions[best], 0, thresholds[best], colors=\"black\",linestyles=\"dashed\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.title(\"Term: \" + TERM + \", Loss: \" + loss_name)\n",
    "plt.savefig(os.path.join(dir_name, TERM + \"_\" + loss_name + \"_precision_curves.jpg\"))\n",
    "plt.show()\n",
    "\n",
    "# Plot - recall curve\n",
    "best = np.argmax(recalls)\n",
    "plt.plot(thresholds, recalls, label=\"recall\", color=\"tab:green\")\n",
    "plt.vlines(thresholds[best], 0, recalls[best], colors=\"black\",linestyles=\"dashed\")\n",
    "plt.hlines(recalls[best], 0, thresholds[best], colors=\"black\",linestyles=\"dashed\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.title(\"Term: \" + TERM + \", Loss: \" + loss_name)\n",
    "plt.savefig(os.path.join(dir_name, TERM + \"_\" + loss_name + \"_recall_curves.jpg\"))\n",
    "plt.show()\n",
    "\n",
    "# Plot - F1 score curve\n",
    "best = np.argmax(F1s)\n",
    "plt.plot(thresholds, F1s, label=\"F1\", color=\"tab:red\")\n",
    "plt.vlines(thresholds[best], 0, F1s[best], colors=\"black\",linestyles=\"dashed\")\n",
    "plt.hlines(F1s[best], 0, thresholds[best], colors=\"black\",linestyles=\"dashed\")\n",
    "plt.ylabel(\"F1 score\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.title(\"Term: \" + TERM + \", Loss: \" + loss_name)\n",
    "plt.savefig(os.path.join(dir_name, TERM + \"_\" + loss_name + \"_F1_curves.jpg\"))\n",
    "plt.show()\n",
    "\n",
    "# Plot - IOU curve\n",
    "best = np.argmax(IOUs)\n",
    "plt.plot(thresholds, IOUs, label=\"IOU\", color=\"tab:purple\")\n",
    "plt.vlines(thresholds[best], 0, IOUs[best], colors=\"black\",linestyles=\"dashed\")\n",
    "plt.hlines(IOUs[best], 0, thresholds[best], colors=\"black\",linestyles=\"dashed\")\n",
    "plt.ylabel(\"IOU\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.title(\"Term: \" + TERM + \", Loss: \" + loss_name)\n",
    "plt.savefig(os.path.join(dir_name, TERM + \"_\" + loss_name + \"_IOU_curves.jpg\"))\n",
    "plt.show()\n",
    "\n",
    "best = np.argmax(means)\n",
    "print(\"The threshold providing the best average performances is: \" + str(thresholds[best]))\n",
    "print(\"Average: \" + str(means[best]))\n",
    "print(\"Precision: \" + str(precisions[best]))\n",
    "print(\"Recall: \" + str(recalls[best]))\n",
    "print(\"F1: \" + str(F1s[best]))\n",
    "print(\"IOUs: \" + str(IOUs[best]))\n",
    "print()\n",
    "\n",
    "best = np.argmax(precisions)\n",
    "print(\"The threshold providing the best precision is: \" + str(thresholds[best]))\n",
    "print(\"Average: \" + str(means[best]))\n",
    "print(\"Precision: \" + str(precisions[best]))\n",
    "print(\"Recall: \" + str(recalls[best]))\n",
    "print(\"F1: \" + str(F1s[best]))\n",
    "print(\"IOUs: \" + str(IOUs[best]))\n",
    "print()\n",
    "\n",
    "best = np.argmax(recalls)\n",
    "print(\"The threshold providing the best recall is: \" + str(thresholds[best]))\n",
    "print(\"Average: \" + str(means[best]))\n",
    "print(\"Precision: \" + str(precisions[best]))\n",
    "print(\"Recall: \" + str(recalls[best]))\n",
    "print(\"F1: \" + str(F1s[best]))\n",
    "print(\"IOUs: \" + str(IOUs[best]))\n",
    "print()\n",
    "\n",
    "best = np.argmax(F1s)\n",
    "print(\"The threshold providing the best F1 is: \" + str(thresholds[best]))\n",
    "print(\"Average: \" + str(means[best]))\n",
    "print(\"Precision: \" + str(precisions[best]))\n",
    "print(\"Recall: \" + str(recalls[best]))\n",
    "print(\"F1: \" + str(F1s[best]))\n",
    "print(\"IOUs: \" + str(IOUs[best]))\n",
    "print()\n",
    "\n",
    "best = np.argmax(IOUs)\n",
    "print(\"The threshold providing the best IOU is: \" + str(thresholds[best]))\n",
    "print(\"Average: \" + str(means[best]))\n",
    "print(\"Precision: \" + str(precisions[best]))\n",
    "print(\"Recall: \" + str(recalls[best]))\n",
    "print(\"F1: \" + str(F1s[best]))\n",
    "print(\"IOUs: \" + str(IOUs[best]))\n",
    "print()\n",
    "\n",
    "curr = time.time()\n",
    "curr = curr - start\n",
    "secondes = curr % 60\n",
    "minutes = (curr-secondes)/60\n",
    "print(\"Computing time: \" + str(minutes) + \" minutes \" + str(secondes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
