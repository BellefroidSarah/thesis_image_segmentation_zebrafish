{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e20ec6cc",
   "metadata": {},
   "source": [
    "# Train - Validate - Evaluate\n",
    "This notebook focuses on the multi-class model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f6df58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import os\n",
    "import random\n",
    "import datasets\n",
    "import metrics\n",
    "import time\n",
    "\n",
    "import constants as cst\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import loss_function_multi_class as loss_fn\n",
    "from unet import UNET\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e0249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_img(model, image, device, transform, out_threshold=0.5):\n",
    "    with torch.no_grad():\n",
    "        x = image\n",
    "        logits = model(x.to(device))\n",
    "        logits = transform(logits)\n",
    "        y_pred = nn.Softmax(dim=1)(logits)\n",
    "        y_size = y_pred.shape \n",
    "        proba = y_pred.detach().cpu().squeeze(0)\n",
    "        \n",
    "        all_out = []\n",
    "        for i in range(y_size[1]): \n",
    "            m = np.zeros((y_size[2], y_size[3]))\n",
    "            m = torch.where(proba[i,:,:]>out_threshold, 1,0)\n",
    "            all_out.append(m)\n",
    "        \n",
    "        all_out = tuple(all_out)\n",
    "        all_out = torch.stack(all_out, 0)\n",
    "        return all_out\n",
    "\n",
    "# Validation step\n",
    "def validate(model, validation_loader, transform, DEVICE, loss_name):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = []\n",
    "        for images, masks, names in validation_loader:\n",
    "            images = transform(images)\n",
    "            outputs = model(images.to(DEVICE))\n",
    "\n",
    "            masks = masks.type(torch.FloatTensor)\n",
    "            masks = transform(masks)\n",
    "            masks = torch.squeeze(masks, 1)\n",
    "            masks = masks.to(DEVICE)\n",
    "            \n",
    "            total_loss = 0\n",
    "            denum = 0\n",
    "            outputs = F.softmax(outputs, dim=1)\n",
    "            sh = images.shape\n",
    "            for batch_n in range(sh[0]):\n",
    "                for i in range(14):\n",
    "                    if masks[batch_n, i,:,:].sum()>0:\n",
    "                        curr_loss = criterion(outputs[batch_n, i,:,:], masks[batch_n, i,:,:])\n",
    "                        total_loss += curr_loss\n",
    "                        denum += 1\n",
    "                \n",
    "            vloss = total_loss/denum\n",
    "            loss = vloss.detach().item()\n",
    "            val_loss.append(loss)\n",
    "\n",
    "        loss = np.mean(val_loss)\n",
    "    return loss\n",
    "\n",
    "# Training step\n",
    "def train(model, training_loader, transforms, DEVICE, criterion, optimiser, loss_name):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for images, masks, names in training_loader:\n",
    "        images = transform(images)\n",
    "        outputs = model(images.to(DEVICE))\n",
    "\n",
    "        masks = masks.type(torch.FloatTensor)\n",
    "        masks = transform(masks)\n",
    "        masks = torch.squeeze(masks, 1)\n",
    "        masks = masks.to(DEVICE)\n",
    "    \n",
    "        outputs = F.softmax(outputs, dim=1)\n",
    "            \n",
    "        total_loss = 0\n",
    "        denum = 0\n",
    "        sh = images.shape\n",
    "        for batch_n in range(sh[0]):\n",
    "            for i in range(14):\n",
    "                #to do \n",
    "                if masks[batch_n, i,:,:].sum()>0:\n",
    "                    curr_loss = criterion(outputs[batch_n,i,:,:], masks[batch_n,i,:,:])\n",
    "                    total_loss += curr_loss\n",
    "                    denum += 1\n",
    "            \n",
    "        tloss = total_loss/denum\n",
    "        loss = tloss.detach().item()\n",
    "        train_loss.append(loss)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        tloss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "    loss = np.mean(train_loss)\n",
    "    return loss\n",
    "\n",
    "# Evaluating performances of the model\n",
    "def evaluate(eval_model, testing_loader, threshold):\n",
    "    tps = 0\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    F1s = []\n",
    "    IOUs = []\n",
    "\n",
    "    eval_model.eval()\n",
    "    for image, mask, name in testing_loader:\n",
    "        prediction = predict_img(eval_model, transform(image), DEVICE, untransform, threshold)\n",
    "        pred_masks = prediction\n",
    "        \n",
    "        p = 0 # precision\n",
    "        r = 0 # recall\n",
    "        f = 0 # f1\n",
    "        iou = 0 # iou\n",
    "        total = 0\n",
    "        for i in range(14):\n",
    "            if mask[:,i,:,:].sum()>0:\n",
    "                p = metrics.precision(pred_masks[i,:,:], mask[:,i,:,:])\n",
    "                r = metrics.recall(pred_masks[i,:,:], mask[:,i,:,:])\n",
    "                f = metrics.F1Score(pred_masks[i,:,:], mask[:,i,:,:])\n",
    "                iou = metrics.IOUScore(pred_masks[i,:,:], mask[:,i,:,:])\n",
    "                total += 1\n",
    "            \n",
    "\n",
    "        precisions.append(p/total)\n",
    "        recalls.append(r/total)\n",
    "        F1s.append(f/total)\n",
    "        IOUs.append(iou/total)\n",
    "    return precisions, recalls, F1s, IOUs\n",
    "\n",
    "# Create one multi-class segmentation mask using multiple masks\n",
    "def stack_masks(masks, size=(512,512)):\n",
    "    stacks = []\n",
    "    tr = transforms.ToTensor()\n",
    "    m_size = masks.shape\n",
    "    for b in range(m_size[0]):\n",
    "        m = np.zeros(size)\n",
    "        m = tr(m)\n",
    "        for c in range(m_size[1]):\n",
    "            m = torch.where(m==0, masks[b,c,:,:]*(c+1),m)\n",
    "        m = m.squeeze()\n",
    "        stacks.append(m)\n",
    "    \n",
    "    stacks = tuple(stacks)\n",
    "    stacks = torch.stack(stacks, 0)\n",
    "    return stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46458195",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(cst.SEED)\n",
    "torch.manual_seed(cst.SEED)\n",
    "np.random.seed(cst.SEED)\n",
    "\n",
    "TERM = \"all_different_masks\"\n",
    "SIZE = (384, 512)  # Related to this project\n",
    "FOLDS = [0,1,2,3,4]  # Allows to run all or some folds (computational resources limits)\n",
    "LOSSES = [\"CE\", \"Dice\", \"Tversky\", \"Focal\"]  # Can study all or some loss functions (computational resources limits)\n",
    "\n",
    "run_name = TERM\n",
    "dir_name = os.path.join(cst.DIR, run_name)\n",
    "os.makedirs(dir_name, exist_ok = True)\n",
    "\n",
    "# Text file log\n",
    "if \"train.txt\" not in os.listdir(dir_name):\n",
    "    f = open(os.path.join(dir_name,\"train.txt\"),\"w+\")\n",
    "    f.write(\"--------------------------------------------------\\n\")\n",
    "    f.write(\"Term studied: \" + TERM + \"\\n\\n\")\n",
    "    f.write(\"--------------------------------------------------\\n\")\n",
    "    f.close()\n",
    "\n",
    "DEVICE_NAME = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE_NAME = 'cuda:0' \n",
    "DEVICE = torch.device(DEVICE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3625a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple loss functions can be studied\n",
    "for loss_name in LOSSES:\n",
    "    print(\"Starting loss:\", loss_name)\n",
    "    # Used for studying the performances using different threshold values\n",
    "    loss_precision = [0] * n_th\n",
    "    loss_recall = [0] * n_th\n",
    "    loss_f1 = [0] * n_th\n",
    "    loss_IOU = [0] * n_th\n",
    "    \n",
    "    print(\"Starting term: \" + TERM)\n",
    "    start_term = time.time()\n",
    "\n",
    "    image_folder = os.path.join(cst.DIR, \"images\")\n",
    "    mask_folder = os.path.join(cst.DIR, TERM)\n",
    "\n",
    "    # Transforms for the images\n",
    "    transform = transforms.Compose([transforms.Resize(SIZE),\n",
    "                                    transforms.Pad((0, 64, 0, 64))])\n",
    "    untransform = transforms.Compose([transforms.CenterCrop(SIZE),\n",
    "                                     transforms.Resize((1932, 2576))])\n",
    "\n",
    "    fold_validation = []\n",
    "    fold_precision = []\n",
    "    fold_recall = []\n",
    "    fold_f1 = []\n",
    "    fold_IOU = []\n",
    "    \n",
    "    fold_precision_argmax = []\n",
    "    fold_recall_argmax = []\n",
    "    fold_f1_argmax = []\n",
    "    fold_IOU_argmax = []\n",
    "\n",
    "    for fold in FOLDS:\n",
    "        print(\"Starting fold: {}\".format(fold))\n",
    "        start_fold = time.time()\n",
    "        \"\"\"Datasets and loaders\"\"\"\n",
    "        training_set = datasets.ZebrafishDataset_multi(actual_fold=fold,\n",
    "                                                      dataset=\"train\",\n",
    "                                                      folds=cst.FOLDS)\n",
    "        validation_set = datasets.ZebrafishDataset_multi(actual_fold=fold,\n",
    "                                                        dataset=\"validate\",\n",
    "                                                        folds=cst.FOLDS)\n",
    "        testing_set = datasets.ZebrafishDataset_multi(actual_fold=fold,\n",
    "                                                     dataset=\"test\",\n",
    "                                                     folds=cst.FOLDS)\n",
    "\n",
    "        training_loader = torch.utils.data.DataLoader(training_set,\n",
    "                                                      batch_size=cst.BATCH_SIZE,\n",
    "                                                      shuffle=True,\n",
    "                                                      num_workers=cst.WORKERS)\n",
    "\n",
    "        validation_loader = torch.utils.data.DataLoader(validation_set,\n",
    "                                                        batch_size=cst.BATCH_SIZE,\n",
    "                                                        shuffle=True,\n",
    "                                                        num_workers=cst.WORKERS)\n",
    "\n",
    "        testing_loader = torch.utils.data.DataLoader(testing_set,\n",
    "                                                     batch_size=1,\n",
    "                                                     shuffle=True,\n",
    "                                                     num_workers=cst.WORKERS)\n",
    "\n",
    "        model = UNET(3, 14)\n",
    "        model.to(DEVICE)\n",
    "        best_model = UNET(3, 14)\n",
    "        best_model = model\n",
    "       \n",
    "        # Text file log - writing loss and parameters\n",
    "        f = open(os.path.join(dir_name,\"train.txt\"),\"a+\")\n",
    "        if fold==0:\n",
    "            f.write(\"Loss used: \" + loss_name + \"\\n\\n\")\n",
    "                \n",
    "        criterion = nn.BCELoss()\n",
    "        criterion_string = loss_name\n",
    "\n",
    "        if loss_name == \"Dice\":\n",
    "            criterion = loss_fn.DiceLoss()\n",
    "        if loss_name == \"Loss used: Tversky\":\n",
    "            A = 0.7\n",
    "            B = 0.3\n",
    "            criterion = loss_fn.TverskyLoss(alpha=A, beta=B)\n",
    "            if fold==0:\n",
    "                f.write(\"Alpha: \" + str(A) + \"\\n\")\n",
    "                f.write(\"Beta: \" + str(B) + \"\\n\\n\")\n",
    "        if loss_name == \"Focal\":\n",
    "            A = 0.8\n",
    "            G = 2\n",
    "            criterion = loss_fn.FocalLoss(alpha=A, gamma=G, reduction=\"mean\")\n",
    "            if fold==0:\n",
    "                f.write(\"Alpha: \" + str(A) + \"\\n\")\n",
    "                f.write(\"Gamma: \" + str(G) + \"\\n\\n\")\n",
    "            \n",
    "        if fold==0:\n",
    "            f.write(\"Learning rate: \" + str(cst.LEARNING_RATE) + \"\\n\")\n",
    "            f.write(\"Weight decay: \" + str(cst.WEIGHT_DECAY) + \"\\n\")\n",
    "            f.write(\"Max epochs: \" + str(cst.EPOCHS) + \"\\n\")\n",
    "            f.write(\"Batch size: \" + str(cst.BATCH_SIZE) + \"\\n\")\n",
    "            f.write(\"Workers: \" + str(cst.WORKERS) + \"\\n\\n\")\n",
    "            f.write(\"--------------------------------------------------\\n\")\n",
    "        f.write(\"Current fold: \" + str(fold) + \"\\n\\n\")\n",
    "        f.close()\n",
    "\n",
    "        optimiser = torch.optim.Adam(model.parameters(), lr=cst.LEARNING_RATE, weight_decay=cst.WEIGHT_DECAY)\n",
    "\n",
    "        \"\"\"Computing validation loss before training\"\"\"\n",
    "        loss = validate(model, validation_loader, transform, DEVICE, loss_name)\n",
    "\n",
    "        best_val = loss\n",
    "        best_epoch = 0\n",
    "        last_epoch = 0\n",
    "\n",
    "        epochs_train_losses = []\n",
    "        epochs_val_losses = []\n",
    "        for epoch in range(cst.EPOCHS):\n",
    "            \"\"\"Training\"\"\"\n",
    "            loss = train(model, training_loader, transforms, DEVICE, criterion, optimiser, loss_name)\n",
    "            epochs_train_losses.append(loss)\n",
    "\n",
    "            \"\"\"Validation\"\"\"\n",
    "            loss = validate(model, validation_loader, transform, DEVICE, loss_name)\n",
    "            epochs_val_losses.append(loss)\n",
    "\n",
    "            \"\"\"Updating best model\"\"\"\n",
    "            if loss < best_val:\n",
    "                best_val = loss\n",
    "                best_model = model\n",
    "                best_epoch = epoch+1\n",
    "\n",
    "            if (epoch+1)%50 == 0:\n",
    "                print(\"Epoch: \" + str(epoch+1))\n",
    "                print(\"Validation: {}.\".format(loss))\n",
    "                print(\"Best validation: {}.\".format(best_val))\n",
    "\n",
    "            \"\"\"Train and validate loops over\"\"\"\n",
    "            curr = time.time()\n",
    "            curr = curr - start_term\n",
    "            secondes = curr % 60\n",
    "            minutes = (curr-secondes)/60\n",
    "\n",
    "            last_epoch = epoch\n",
    "\n",
    "            # Notebooks shutdown after 6 hours. Stop the code and save the results.\n",
    "            if minutes >= 350:\n",
    "                f = open(os.path.join(dir_name,\"train.txt\"),\"a+\")\n",
    "                f.write(\"Learning stopped due to timeout. \\n\")\n",
    "                f.close()\n",
    "                break\n",
    "            if (epoch - best_epoch) >= 50:\n",
    "                break\n",
    "\n",
    "        \"\"\"All epochs are over\"\"\"\n",
    "        fold_validation.append(best_val)\n",
    "\n",
    "        model_name = TERM + '_' + loss_name + \"_Fold_\" + str(fold) + \"_Epoch_\" + str(best_epoch) + \"_MaxEpochs_\" \n",
    "        model_name += str(cst.EPOCHS) + '_' + cst.OPTIMIZER + \"_LR_\" + str(cst.LEARNING_RATE) + \".pth\"\n",
    "\n",
    "        model_filepath = os.path.join(dir_name, model_name)\n",
    "        torch.save(best_model.state_dict(), model_filepath) # save better?\n",
    "        \n",
    "        curr = time.time()\n",
    "        curr = curr - start_fold\n",
    "        secondes = curr % 60\n",
    "        minutes = (curr-secondes)/60\n",
    "\n",
    "        # Plot losses\n",
    "        index = [i+1 for i in range(last_epoch+1)]\n",
    "        plt.plot(index[1:], epochs_train_losses[1:], label=\"Training\")\n",
    "        plt.plot(index[1:], epochs_val_losses[1:], label=\"Validation\")\n",
    "        plt.title(\"Term: \" + TERM + \", Loss: \" + loss_name + \", Fold: \" + str(fold)) \n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(dir_name, TERM + \"_\" + loss_name +\"_Fold_\" + str(fold) + \"_Loss_curves.jpg\"))\n",
    "        plt.show()\n",
    "        \n",
    "        # Text file log\n",
    "        f = open(os.path.join(dir_name,\"train.txt\"),\"a+\")\n",
    "        f.write(\"Best epoch: \" + str(best_epoch) + \"\\n\")\n",
    "        f.write(\"Best validation loss: \" + str(best_val) + \"\\n\")\n",
    "        f.write(\"Ellapsed time: \" + str(minutes) + \" minutes \" + str(secondes) + \" seconds\\n\\n\")\n",
    "        f.write(\"Name of the model saved:\\n\")\n",
    "        f.write(model_name + \"\\n\\n\")\n",
    "        f.close()\n",
    "\n",
    "        print(\"Fold took: \" + str(minutes) + \" minutes \" + str(secondes) + \" seconds to train\")\n",
    "        \n",
    "        f = open(os.path.join(dir_name,\"train.txt\"),\"a+\")\n",
    "        f.write(\"Total fold time: \" + str(minutes) + \" minutes \" + str(secondes) + \" seconds\\n\\n\")\n",
    "        f.write(\"--------------------------------------------------\\n\")\n",
    "        f.close()\n",
    "        \n",
    "    \"\"\"Fold loop end\"\"\"\n",
    "    \n",
    "\"\"\"term loop end\"\"\"\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
