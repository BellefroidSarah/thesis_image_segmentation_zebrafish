{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11cbec80",
   "metadata": {},
   "source": [
    "# Creating segmenation masks \n",
    "### Creating masks using the model that crops the image around the head of the fish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94d77f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import os\n",
    "import random\n",
    "import metrics\n",
    "import time\n",
    "\n",
    "import constants as cst\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.ops import masks_to_boxes\n",
    "from torchvision.utils import draw_segmentation_masks\n",
    "\n",
    "from unet import UNET\n",
    "import utils\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9f5d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_img(model, image, device, transform, out_threshold=0.5):\n",
    "    with torch.no_grad():\n",
    "        x = image\n",
    "        logits = model(x.to(device))\n",
    "        logits = transform(logits)\n",
    "        y_pred = nn.Softmax(dim=1)(logits)\n",
    "        proba = y_pred.detach().cpu().squeeze(0).numpy()[1, :, :]\n",
    "        return proba > out_threshold\n",
    "\n",
    "\n",
    "TERMS = cst.COMBINED_TERM\n",
    "    \n",
    "# Images that belong to every testing set\n",
    "names = [\"120220_4xzoom_v_fish 16.jpg\",\n",
    "        \"120220_4xzoom_v2_fish 14.jpg\",\n",
    "        \"26022020 inx +- 4xzoom fish08 v3.jpg\",\n",
    "        \"26022020 inx +- 4xzoom fish11 v.jpg\",\n",
    "        \"120220_4xzoom_v_fish 01.jpg\",\n",
    "        \"120220_4xzoom_v_fish 09.jpg\",\n",
    "        \"120220_4xzoom_v1_fish 19.jpg\",\n",
    "        \"120220_4xzoom_v2_fish 15.jpg\",\n",
    "        \"120220_4xzoom_v2_fish 17.jpg\",\n",
    "        \"120220_4xzoom_v1_fish 23.jpg\",\n",
    "        \"120220_4xzoom_v2_fish 21.jpg\",\n",
    "        \"120220_4xzoom_v1_fish 11.jpg\"]\n",
    "\n",
    "\n",
    "img_names = [\"img1\",\n",
    "             \"img2\",\n",
    "             \"img3\",\n",
    "             \"img4\",\n",
    "             \"img5\",\n",
    "             \"img6\",\n",
    "             \"img7\",\n",
    "             \"img8\",\n",
    "             \"img9\",\n",
    "             \"img10\",\n",
    "             \"img11\",\n",
    "             \"img12\"]\n",
    "\n",
    "random.seed(cst.SEED)\n",
    "torch.manual_seed(cst.SEED)\n",
    "np.random.seed(cst.SEED)\n",
    "\n",
    "# One model per annotation\n",
    "model_names = [\"br1_Focal_Fold_2_Epoch_95_MaxEpochs_250_Adam_LR_0.0001.pth\",\n",
    "              \"br2_CE_Fold_2_Epoch_225_MaxEpochs_250_Adam_LR_0.0001.pth\",\n",
    "              \"cb_Tversky_Fold_3_Epoch_97_MaxEpochs_250_Adam_LR_0.0001.pth\",\n",
    "              \"ch_Tversky_Fold_2_Epoch_190_MaxEpochs_250_Adam_LR_0.0001.pth\",\n",
    "              \"cl_Focal_Fold_2_Epoch_47_MaxEpochs_250_Adam_LR_0.0001.pth\",\n",
    "              \"d_CE_Fold_3_Epoch_194_MaxEpochs_250_Adam_LR_0.0001.pth\",\n",
    "              \"en_Tversky_Fold_4_Epoch_105_MaxEpochs_250_Adam_LR_0.0001.pth\",\n",
    "              \"hm_Tversky_Fold_2_Epoch_176_MaxEpochs_250_Adam_LR_0.0001.pth\",\n",
    "              \"m_CE_Fold_4_Epoch_247_MaxEpochs_250_Adam_LR_0.0001.pth\",\n",
    "              \"n_Focal_Fold_3_Epoch_96_MaxEpochs_250_Adam_LR_0.0001.pth\",\n",
    "              \"oc_Focal_Fold_0_Epoch_47_MaxEpochs_250_Adam_LR_0.0001.pth\",\n",
    "              \"op_Focal_Fold_3_Epoch_57_MaxEpochs_250_Adam_LR_0.0001.pth\",\n",
    "              \"p_CE_Fold_3_Epoch_129_MaxEpochs_250_Adam_LR_0.0001.pth\"]\n",
    "\n",
    "\n",
    "model_path = os.path.join(cst.DIR, \"final_cropped\")\n",
    "save_path = os.path.join(cst.DIR, \"pred_cropped\")\n",
    "\n",
    "# Threshold for each model/annotation\n",
    "thresholds = [0.06,\n",
    "              0.02,\n",
    "              0.02,\n",
    "              0.02,\n",
    "              0.08,\n",
    "              0.02,\n",
    "              0.02,\n",
    "              0.0,\n",
    "              0.02,\n",
    "              0.1,\n",
    "              0.08,\n",
    "              0.08,\n",
    "              0.08]\n",
    "\n",
    "# Color for each annotation\n",
    "all_colors = [\"red\",\n",
    "          \"coral\",\n",
    "          \"sandybrown\",\n",
    "          \"gold\",\n",
    "          \"greenyellow\",\n",
    "          \"seagreen\",\n",
    "          \"cyan\",\n",
    "          \"steelblue\",\n",
    "          \"blue\",\n",
    "          \"mediumslateblue\",\n",
    "          \"darkorchid\",\n",
    "          \"magenta\",\n",
    "          \"lightpink\",\n",
    "          \"dimgrey\"]\n",
    "\n",
    "loss_name = \"best\"\n",
    "fold = 0\n",
    "\n",
    "SIZE = (384, 512)\n",
    "\n",
    "\n",
    "DEVICE_NAME = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE_NAME = 'cuda:0'\n",
    "DEVICE = torch.device(DEVICE_NAME)\n",
    "\n",
    "transform_tensor = transforms.ToTensor()\n",
    "transform = transforms.Compose([transforms.Resize(SIZE),\n",
    "                                transforms.Pad((0, 64, 0, 64))])\n",
    "untransform = transforms.Compose([transforms.CenterCrop(SIZE),\n",
    "                                 transforms.Resize((1932, 2576))])\n",
    "\n",
    "# Model used to segment the fish out of the image\n",
    "fish_model_name = \"fish_CE_Fold_4_Epoch_95_MaxEpochs_600_Adam_LR_0.0001.pth\"\n",
    "fish_model_path = os.path.join(cst.MODEL, fish_model_name)\n",
    "\n",
    "fish_model = utils.load_model(fish_model_path)\n",
    "fish_model.to(DEVICE)\n",
    "\n",
    "for i in range(len(names)):\n",
    "    name_img = names[i]\n",
    "    save_name = img_names[i]\n",
    "    \n",
    "    img_path = os.path.join(cst.DIR, \"images\")\n",
    "    \n",
    "    img = transform_tensor(Image.open(os.path.join(img_path, name_img)))\n",
    "    img = img[:3,:,:]\n",
    "    \n",
    "    fish_image = transform(img)\n",
    "    fish_mask = predict_img(fish_model, fish_image.unsqueeze(dim=0), DEVICE, untransform)\n",
    "    fish_mask_image = Image.fromarray(fish_mask)\n",
    "    fish_mask_tensor = transform_tensor(fish_mask_image)\n",
    "    obj_ids = torch.unique(fish_mask_tensor)\n",
    "    obj_ids = obj_ids[1:]\n",
    "\n",
    "    fish_masks = fish_mask_tensor == obj_ids[:, None, None]\n",
    "    boxes = masks_to_boxes(fish_masks)\n",
    "\n",
    "    h_length = boxes[0, 2]+1 - boxes[0, 0]\n",
    "    v_length = boxes[0, 3]+1 - boxes[0, 1]\n",
    "    h1 = int(boxes[0, 0])\n",
    "    h2 = int(boxes[0, 2])+1\n",
    "    v1 = int(boxes[0, 1])\n",
    "    v2 = int(boxes[0, 3])+1\n",
    "    if h_length%10!=0:\n",
    "        mod = 10 - (h_length%4)\n",
    "        h_length += mod\n",
    "\n",
    "    h_length = (3*h_length)/5\n",
    "    h2 = int(h1 + h_length)\n",
    "\n",
    "    if v_length%2==1:\n",
    "        v1 = v1-1\n",
    "        v_length += 1\n",
    "        \n",
    "    if h_length>v_length:\n",
    "        padding = int((h_length-v_length)/2)\n",
    "        post_tr = transforms.Compose([transforms.Pad((0, padding, 0, padding)),\n",
    "                                      transforms.Resize((512,512))])\n",
    "        untr = transforms.Compose([transforms.Resize((int(h_length), int(h_length))),\n",
    "                                  transforms.CenterCrop((int(v_length), int(h_length)))])\n",
    "    elif h_length<v_length:\n",
    "        padding = int((v_length-h_length)/2)\n",
    "        post_tr = transforms.Compose([transforms.Pad((padding, 0, padding, 0)),\n",
    "                                      transforms.Resize((512,512))])\n",
    "        untr = transforms.Compose([transforms.Resize((int(v_length), int(v_length))),\n",
    "                                   transforms.CenterCrop((int(v_length), int(h_length)))])\n",
    "    else:\n",
    "        post_tr = transforms.Compose([transforms.Resize((512,512))])\n",
    "        untr = transforms.Compose([transforms.Resize((int(h_length), int(h_length)))])\n",
    "        \n",
    "    cropped = img[:, v1:v2, h1:h2]    \n",
    "    img_copy = cropped\n",
    "    \n",
    "    img = cropped.unsqueeze(dim=0)\n",
    "    \n",
    "    \n",
    "    all_groundtruth = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    for t in range(len(TERMS)-1):\n",
    "        print(\"Image:\" , name_img)\n",
    "        print(\"Model:\", model_names[t])\n",
    "        print(\"Threshold:\", thresholds[t])\n",
    "        model_name = model_names[t]\n",
    "        model = utils.load_model(os.path.join(model_path, model_names[t]))\n",
    "        model.to(DEVICE)\n",
    "        \n",
    "        grdtruth = transform_tensor(Image.open(os.path.join(cst.DIR, TERMS[t],name_img)))\n",
    "        grdtruth = grdtruth[:, v1:v2, h1:h2]  \n",
    "        plt.imshow(grdtruth.squeeze())\n",
    "        plt.axis('off')\n",
    "        plt.savefig(os.path.join(save_path, save_name + \"_gt_\" + TERMS[t] + \".jpg\"))\n",
    "        plt.show()\n",
    "        grdtruth = grdtruth.unsqueeze(dim=0)\n",
    "        all_groundtruth.append(grdtruth)\n",
    "        \n",
    "        prediction = predict_img(model, post_tr(img), DEVICE, untr, out_threshold=thresholds[t])\n",
    "        pred = torch.from_numpy(prediction)\n",
    "        all_predictions.append(pred)\n",
    "        plt.imshow(pred)\n",
    "        plt.axis('off')\n",
    "        plt.savefig(os.path.join(save_path, save_name + \"_pred_\" + loss_name + \"_\"+ TERMS[t] + \".jpg\"))\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    img_copy = img_copy * 255\n",
    "    img_copy = img_copy.type(torch.uint8)\n",
    "    img_copy2 = img_copy\n",
    "    \n",
    "    im = img_copy\n",
    "    im_gt = im\n",
    "    drawn_masks = []\n",
    "    for p in range(len(all_predictions)):\n",
    "        if p == 7:\n",
    "            gt = all_groundtruth[p].squeeze(dim=0)\n",
    "            gt_bool = gt > 0.9 \n",
    "            im_gt = draw_segmentation_masks(im_gt, gt_bool[0], alpha=1, colors=all_colors[p])\n",
    "            continue\n",
    "            \n",
    "        predict = all_predictions[p]\n",
    "        predict = predict.unsqueeze(dim=0)\n",
    "        \n",
    "        gt = all_groundtruth[p].squeeze(dim=0)\n",
    "        gt_bool = gt > 0.9        \n",
    "        \n",
    "        im = draw_segmentation_masks(im, predict[0], alpha=1, colors=all_colors[p])\n",
    "        im_gt = draw_segmentation_masks(im_gt, gt_bool[0], alpha=1, colors=all_colors[p])\n",
    "        \n",
    "    plt.imshow(im.permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.savefig(os.path.join(save_path, \"0_\" + save_name + \"_pred_on_img_\" + loss_name + \".jpg\"))\n",
    "    plt.show()\n",
    "    \n",
    "    plt.imshow(im_gt.permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.savefig(os.path.join(save_path, \"0_\" + save_name + \"_gt_on_img_\" + loss_name + \".jpg\"))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
